{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "0.596951961517334\n",
      "512\n",
      "0.026210546493530273\n",
      "512\n",
      "0.251232385635376\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import time \n",
    "import clip\n",
    "from transformers import AutoProcessor, CLIPModel, CLIPProcessor\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "import json\n",
    "\n",
    "\n",
    "def get_image_embedding1(image_path, model, processor,device):\n",
    "\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path)\n",
    "    #image = transform(image).unsqueeze(0).to(device)\n",
    "    # Generate the image embedding\n",
    "    with torch.no_grad():\n",
    "        image_inputs = processor(images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        image_inputs = {name: tensor.to(device) for name, tensor in image_inputs.items()}\n",
    "        image_embeddings = model.to(device).get_image_features(**image_inputs)\n",
    "    # Convert the tensor to a list\n",
    "    image_embeddings_list = image_embeddings.tolist()\n",
    "    #image_embeddings_list = image_embeddings.cpu().tolist()\n",
    "    with open(\"embeddings1.json\", \"w\") as outfile:\n",
    "        json.dump(image_embeddings_list[0], outfile)\n",
    "    print(len(image_embeddings_list[0]))\n",
    "    return image_embeddings_list[0]\n",
    "\n",
    "def get_image_embedding2(image_path, model, processor,device):\n",
    "\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path)\n",
    "    # Generate the image embedding\n",
    "    with torch.no_grad():\n",
    "        image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        image_embeddings = model.get_image_features(**image_inputs)\n",
    "    # Convert the tensor to a list\n",
    "    image_embeddings_list = image_embeddings.tolist()\n",
    "    #image_embeddings_list = image_embeddings.cpu().tolist()\n",
    "    with open(\"embeddings2.json\", \"w\") as outfile:\n",
    "        json.dump(image_embeddings_list[0], outfile)\n",
    "    print(len(image_embeddings_list[0]))\n",
    "    return image_embeddings_list[0]\n",
    "\n",
    "def get_image_embedding3(image_path, model, processor,device):\n",
    "\n",
    "    # Generate the image embedding\n",
    "    with torch.no_grad():\n",
    "        image_preprocess = processor(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        image_embeddings = model.encode_image( image_preprocess)\n",
    "    # Convert the tensor to a list\n",
    "    image_embeddings_list = image_embeddings.tolist()\n",
    "    #image_embeddings_list = image_embeddings.cpu().tolist()\n",
    "    with open(\"embeddings3.json\", \"w\") as outfile:\n",
    "        json.dump(image_embeddings_list[0], outfile)\n",
    "    print(len(image_embeddings_list[0]))\n",
    "    return image_embeddings_list[0]\n",
    "\n",
    "\n",
    "\n",
    "#Input image\n",
    "source='./unsplash-main/ZZw-8XuYs0s.jpg'\n",
    "\n",
    "processor_auto = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor_clip1 = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model_clip1 = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "model_clip2, processor_clip2 = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "get_image_embedding1( source, model_clip1, processor_clip1, device)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "get_image_embedding2( source, model_clip1, processor_auto, device)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "get_image_embedding3( source, model_clip2, processor_clip2, device)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from opensearchpy import helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "SERVER_URL = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"unsplash_knn_bulk_hybrid_multi_modal_index_03\"\n",
    "\n",
    "UNSPLASH_METADATA_PATH = \"./meta_data.json\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    return data / np.linalg.norm(data, ord=2)\n",
    "\n",
    "\n",
    "def load_file(file_path):\n",
    "    try:\n",
    "        json_objects = []\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            for line in json_file:\n",
    "                data = json.loads(line)\n",
    "                json_objects.append(data)\n",
    "        print(\"Done\")\n",
    "    finally:\n",
    "        json_file.close()\n",
    "    return json_objects\n",
    "\n",
    "\n",
    "def get_client(server_url: str) -> OpenSearch:\n",
    "    os_client_instance = OpenSearch('http://localhost:9200', use_ssl=False, verify_certs=False,\n",
    "                                    connection_class=RequestsHttpConnection)\n",
    "    print(\"OS connected\")\n",
    "    print(datetime.datetime.now())\n",
    "    return os_client_instance\n",
    "\n",
    "\n",
    "def create_index(index_name: str, os_client: OpenSearch, metadata: np):\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"asin\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                },\n",
    "                \"text_field\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword_field\": {\n",
    "                            \"type\": \"keyword\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"description_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": get_vector_dimension(metadata),\n",
    "                },\n",
    "                \"item_image\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                },\n",
    "                \"image_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 512\n",
    "                }\n",
    "\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"number_of_shards\": \"1\",\n",
    "                \"knn\": \"false\",\n",
    "                \"number_of_replicas\": \"1\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    }\n",
    "    os_client.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "\n",
    "def delete_index(index_name: str, os_client: OpenSearch):\n",
    "    os_client.indices.delete(index_name)\n",
    "\n",
    "\n",
    "def get_vector_dimension(metadata: list):\n",
    "    meta_data = metadata[0][\"meta_data\"]\n",
    "    embeddings = model.encode(meta_data)\n",
    "    return len(embeddings)\n",
    "\n",
    "def get_image_embedding(image):\n",
    "    image = preprocess(Image.open(image)).unsqueeze(0).to(device)\n",
    "    # Generate the image embedding\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model_clip.encode_image(image)\n",
    "        image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    image_embedding = image_embedding.squeeze().cpu().tolist()\n",
    "\n",
    "    return image_embedding\n",
    "\n",
    "def store_index(index_name: str, data: np.array, metadata: list, os_client: OpenSearch):\n",
    "    documents = []\n",
    "    for index_num, vector in enumerate(data,start=1):\n",
    "        metadata_line = metadata[index_num]\n",
    "        text_field = metadata_line[\"meta_data\"]\n",
    "        embedding = model.encode(text_field)\n",
    "        norm_text_vector_np = normalize_data(embedding)\n",
    "        image_id = metadata_line[\"photo_id\"]\n",
    "        image_file = f\"/home/shivazi/Desktop/ai-rnd/vector-data/dataset/unsplash/unsplash_25k/{image_id}.jpg\"\n",
    "\n",
    "        if not os.path.exists(image_file):\n",
    "            image_embedding = [0] * 512\n",
    "        else:\n",
    "            image_embedding = get_image_embedding(image_file)\n",
    "\n",
    "        document = {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": index_num,\n",
    "            \"asin\": metadata_line[\"photo_id\"],\n",
    "            \"text_field\": text_field,\n",
    "            \"description_vector\": norm_text_vector_np.tolist(),\n",
    "            \"item_image\": metadata_line[\"photo_url\"],\n",
    "            \"image_vector\": image_embedding\n",
    "        }\n",
    "        documents.append(document)\n",
    "        if index_num % 1000 == 0 or index_num == len(data):\n",
    "            helpers.bulk(os_client, documents, request_timeout=1800)\n",
    "            documents = []\n",
    "            print(f\"bulk {index_num} indexed successfully\")\n",
    "            os_client.indices.refresh(INDEX_NAME)\n",
    "\n",
    "    os_client.indices.refresh(INDEX_NAME)\n",
    "\n",
    "\n",
    "def main():\n",
    "    os_client = get_client(SERVER_URL)\n",
    "    metadata = load_file(UNSPLASH_METADATA_PATH)\n",
    "    create_index(INDEX_NAME, os_client, metadata)\n",
    "    store_index(INDEX_NAME, metadata, metadata, os_client)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
